{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORSufKYbF0MRs226g9qsJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrideep-tamboli/Text-Analysis/blob/main/Blackcoffer_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web Scraping and Text Analysis"
      ],
      "metadata": {
        "id": "bdPce-XIwMMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Setup"
      ],
      "metadata": {
        "id": "6PMnxg602GoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR1RML4YXQQX",
        "outputId": "9fd8943f-e18e-4f7c-c9e4-ecff2da73868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loading the input.xlsx file with all the URLs"
      ],
      "metadata": {
        "id": "bCaZA_QGp1aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "url_file_path = '/content/gdrive/MyDrive/Blackcoffer/Input.xlsx'\n",
        "url_input = openpyxl.load_workbook(url_file_path)\n",
        "sheet = url_input.active"
      ],
      "metadata": {
        "id": "07EQT72Hpmja"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing the Stopwords Bag of words"
      ],
      "metadata": {
        "id": "Wj4VnRsYuSd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define the path to the StopWords folder\n",
        "stopwords_folder = '/content/gdrive/MyDrive/Blackcoffer/StopWords'\n",
        "\n",
        "# Get the list of stop words files in the folder\n",
        "stopwords_files = [f for f in os.listdir(stopwords_folder) if f.endswith('.txt')]\n",
        "\n",
        "# Create a set to store all the stop words\n",
        "stopwords1 = set()\n",
        "\n",
        "# Read the stop words from each file and add them to the set\n",
        "for file_name in stopwords_files:\n",
        "    file_path = os.path.join(stopwords_folder, file_name)\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        stopword_list = [word.lower() for word in file.read().split()]\n",
        "        stopwords1.update(stopword_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w1_EcfWA7l_G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Web Scraping,\n",
        "# 3. Data Cleaning"
      ],
      "metadata": {
        "id": "FmWLgYb_uv22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Download the stop words list\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Iterate over each row in the sheet to get url and url_id\n",
        "for row in sheet.iter_rows(min_row=2, values_only=True):\n",
        "    url_id = row[0]\n",
        "    url = row[1]\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    URL_content = response.text\n",
        "\n",
        "    #-----------------------------\n",
        "    #--------Web Scraping---------\n",
        "    #-----------------------------\n",
        "\n",
        "    # Create a BeautifulSoup object to parse the HTML\n",
        "    soup = BeautifulSoup(URL_content, 'html.parser')\n",
        "\n",
        "    # Find the article title and text elements\n",
        "    title_element = soup.find('h1')\n",
        "    text_elements = soup.find_all('p')\n",
        "\n",
        "    # Extract the text from the elements and the title\n",
        "    title = title_element.get_text() if title_element else ''\n",
        "    text = ' '.join(element.get_text() for element in text_elements)\n",
        "\n",
        "    # Remove unwanted text\n",
        "    unwanted_text1 = \"We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-level human expertise. Contact us: hello@blackcoffer.com\"\n",
        "    unwanted_text2 = \"© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\"\n",
        "    unwanted_text3 = \"Ranking customer behaviours for business strategy Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc. Trading Bot for FOREX Python model for the analysis of sector-specific stock ETFs for investment purposes AutoGPT Setup Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard Google Local Service Ads LSA API To Google BigQuery to Google Data Studio AI Conversational Bot using RASA Rise of telemedicine and its Impact on Livelihood by 2040 Rise of e-health and its impact on humans by the year 2030 Rise of e-health and its impact on humans by the year 2030 Rise of telemedicine and its Impact on Livelihood by 2040 AI/ML and Predictive Modeling Solution for Contact Centre Problems How to Setup Custom Domain for Google App Engine Application? Code Review Checklist\"\n",
        "\n",
        "    text = text.replace(unwanted_text1, \"\")\n",
        "    text = text.replace(unwanted_text2, \"\")\n",
        "    text = text.replace(unwanted_text3, \"\")\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenized_text = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # Preprocessing: Remove unwanted characters using regex\n",
        "    filtered_text = [re.sub(r'[^\\w\\s]', '', word) for word in tokenized_text]\n",
        "    filtered_text = [re.sub(r'\\d+', '', word) for word in filtered_text]\n",
        "\n",
        "    #-----------------------------------\n",
        "    #-----------Data Cleaning-----------\n",
        "    #-----------------------------------\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_text = [word for word in filtered_text if word.lower() not in stopwords1]\n",
        "\n",
        "    # Join the filtered text back into a string\n",
        "    cleaned_text = ' '.join(filtered_text)\n",
        "\n",
        "    # Save the extracted article in a text file\n",
        "    file_name = f'{url_id}.txt'\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.write(f'{title}\\n\\n{cleaned_text}')\n",
        "\n",
        "    print(f'Saved article {url_id} to {file_name}')\n"
      ],
      "metadata": {
        "id": "X4xHgeYh_QCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7411a9b-9e6c-48f9-bf5b-ceb9aa8fb670"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved article 123.0 to 123.0.txt\n",
            "Saved article 321.0 to 321.0.txt\n",
            "Saved article 2345.0 to 2345.0.txt\n",
            "Saved article 4321.0 to 4321.0.txt\n",
            "Saved article 432.0 to 432.0.txt\n",
            "Saved article 2893.8 to 2893.8.txt\n",
            "Saved article 3355.6 to 3355.6.txt\n",
            "Saved article 3817.4 to 3817.4.txt\n",
            "Saved article 4279.2 to 4279.2.txt\n",
            "Saved article 4741.0 to 4741.0.txt\n",
            "Saved article 5202.8 to 5202.8.txt\n",
            "Saved article 5664.6 to 5664.6.txt\n",
            "Saved article 6126.4 to 6126.4.txt\n",
            "Saved article 6588.2 to 6588.2.txt\n",
            "Saved article 7050.0 to 7050.0.txt\n",
            "Saved article 7511.8 to 7511.8.txt\n",
            "Saved article 7973.6 to 7973.6.txt\n",
            "Saved article 8435.4 to 8435.4.txt\n",
            "Saved article 8897.2 to 8897.2.txt\n",
            "Saved article 9359.0 to 9359.0.txt\n",
            "Saved article 9820.8 to 9820.8.txt\n",
            "Saved article 10282.6 to 10282.6.txt\n",
            "Saved article 10744.4 to 10744.4.txt\n",
            "Saved article 11206.2 to 11206.2.txt\n",
            "Saved article 11668.0 to 11668.0.txt\n",
            "Saved article 12129.8 to 12129.8.txt\n",
            "Saved article 12591.6 to 12591.6.txt\n",
            "Saved article 13053.4 to 13053.4.txt\n",
            "Saved article 13515.2 to 13515.2.txt\n",
            "Saved article 13977.0 to 13977.0.txt\n",
            "Saved article 14438.8 to 14438.8.txt\n",
            "Saved article 14900.6 to 14900.6.txt\n",
            "Saved article 15362.4 to 15362.4.txt\n",
            "Saved article 15824.2 to 15824.2.txt\n",
            "Saved article 16286.0 to 16286.0.txt\n",
            "Saved article 16747.8 to 16747.8.txt\n",
            "Saved article 17209.6 to 17209.6.txt\n",
            "Saved article 17671.4 to 17671.4.txt\n",
            "Saved article 18133.2 to 18133.2.txt\n",
            "Saved article 18595.0 to 18595.0.txt\n",
            "Saved article 19056.8 to 19056.8.txt\n",
            "Saved article 19518.6 to 19518.6.txt\n",
            "Saved article 19980.4 to 19980.4.txt\n",
            "Saved article 20442.2 to 20442.2.txt\n",
            "Saved article 20904.0 to 20904.0.txt\n",
            "Saved article 21365.8 to 21365.8.txt\n",
            "Saved article 21827.6 to 21827.6.txt\n",
            "Saved article 22289.4 to 22289.4.txt\n",
            "Saved article 22751.2 to 22751.2.txt\n",
            "Saved article 23213.0 to 23213.0.txt\n",
            "Saved article 23674.8 to 23674.8.txt\n",
            "Saved article 24136.6 to 24136.6.txt\n",
            "Saved article 24598.4 to 24598.4.txt\n",
            "Saved article 25060.2 to 25060.2.txt\n",
            "Saved article 25522.0 to 25522.0.txt\n",
            "Saved article 25983.8 to 25983.8.txt\n",
            "Saved article 26445.6 to 26445.6.txt\n",
            "Saved article 26907.4 to 26907.4.txt\n",
            "Saved article 27369.2 to 27369.2.txt\n",
            "Saved article 27831.0 to 27831.0.txt\n",
            "Saved article 28292.8 to 28292.8.txt\n",
            "Saved article 28754.6 to 28754.6.txt\n",
            "Saved article 29216.4 to 29216.4.txt\n",
            "Saved article 29678.2 to 29678.2.txt\n",
            "Saved article 30140.0 to 30140.0.txt\n",
            "Saved article 30601.8 to 30601.8.txt\n",
            "Saved article 31063.6 to 31063.6.txt\n",
            "Saved article 31525.4 to 31525.4.txt\n",
            "Saved article 31987.2 to 31987.2.txt\n",
            "Saved article 32449.0 to 32449.0.txt\n",
            "Saved article 32910.8 to 32910.8.txt\n",
            "Saved article 33372.6 to 33372.6.txt\n",
            "Saved article 33834.4 to 33834.4.txt\n",
            "Saved article 34296.2 to 34296.2.txt\n",
            "Saved article 34758.0 to 34758.0.txt\n",
            "Saved article 35219.8 to 35219.8.txt\n",
            "Saved article 35681.6 to 35681.6.txt\n",
            "Saved article 36143.4 to 36143.4.txt\n",
            "Saved article 36605.2 to 36605.2.txt\n",
            "Saved article 37067.0 to 37067.0.txt\n",
            "Saved article 37528.8 to 37528.8.txt\n",
            "Saved article 37990.6 to 37990.6.txt\n",
            "Saved article 38452.4 to 38452.4.txt\n",
            "Saved article 38914.2 to 38914.2.txt\n",
            "Saved article 39376.0 to 39376.0.txt\n",
            "Saved article 39837.8 to 39837.8.txt\n",
            "Saved article 40299.6 to 40299.6.txt\n",
            "Saved article 40761.4 to 40761.4.txt\n",
            "Saved article 41223.2 to 41223.2.txt\n",
            "Saved article 41685.0 to 41685.0.txt\n",
            "Saved article 42146.8 to 42146.8.txt\n",
            "Saved article 42608.6 to 42608.6.txt\n",
            "Saved article 43070.4 to 43070.4.txt\n",
            "Saved article 43532.2 to 43532.2.txt\n",
            "Saved article 43994.0 to 43994.0.txt\n",
            "Saved article 44455.8 to 44455.8.txt\n",
            "Saved article 44917.6 to 44917.6.txt\n",
            "Saved article 45379.4 to 45379.4.txt\n",
            "Saved article 45841.2 to 45841.2.txt\n",
            "Saved article 46303.0 to 46303.0.txt\n",
            "Saved article 46764.8 to 46764.8.txt\n",
            "Saved article 47226.6 to 47226.6.txt\n",
            "Saved article 47688.4 to 47688.4.txt\n",
            "Saved article 48150.2 to 48150.2.txt\n",
            "Saved article 48612.0 to 48612.0.txt\n",
            "Saved article 49073.8 to 49073.8.txt\n",
            "Saved article 49535.6 to 49535.6.txt\n",
            "Saved article 49997.4 to 49997.4.txt\n",
            "Saved article 50459.2 to 50459.2.txt\n",
            "Saved article 50921.0 to 50921.0.txt\n",
            "Saved article 51382.8 to 51382.8.txt\n",
            "Saved article 51844.6 to 51844.6.txt\n",
            "Saved article 52306.4 to 52306.4.txt\n",
            "Saved article 52768.2 to 52768.2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyphen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeqBBP6_zqvf",
        "outputId": "edf4e5bd-a2d0-48c8-faad-c8f924ee3dc0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen\n",
            "Successfully installed pyphen-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Text Analysis"
      ],
      "metadata": {
        "id": "PRSsFLMm-Xme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import re\n",
        "import nltk\n",
        "import pyphen\n",
        "\n",
        "positive_dict_path = '/content/gdrive/MyDrive/Blackcoffer/MasterDictionary/positive-words.txt'\n",
        "negative_dict_path = '/content/gdrive/MyDrive/Blackcoffer/MasterDictionary/negative-words.txt'\n",
        "\n",
        "positive_words = set()\n",
        "negative_words = set()\n",
        "\n",
        "# Read positive words from the dictionary file\n",
        "with open(positive_dict_path, 'r', encoding='latin-1') as file:\n",
        "    positive_words = {word.strip().lower() for word in file}\n",
        "\n",
        "# Read negative words from the dictionary file\n",
        "with open(negative_dict_path, 'r', encoding='latin-1') as file:\n",
        "    negative_words = {word.strip().lower() for word in file}\n",
        "\n",
        "# Create a Pyphen instance for syllable counting\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "\n",
        "results = []\n",
        "\n",
        "# Get the list of saved .txt files\n",
        "txt_files = glob.glob('*.txt')\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    dic = pyphen.Pyphen(lang='en')\n",
        "    syllables = len(dic.inserted(word).split('-'))\n",
        "    return syllables\n",
        "\n",
        "# Regular expression pattern to match personal pronouns\n",
        "personal_pronoun_pattern = re.compile(r'\\b(I|we|my|ours|us)\\b', re.IGNORECASE)\n",
        "\n",
        "# Iterate through the saved .txt files\n",
        "for file_name in txt_files:\n",
        "    url_id = int(file_name.split('.')[0])\n",
        "\n",
        "    with open(file_name, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Extract the title and cleaned_text from the file\n",
        "    title = lines[0].strip()\n",
        "    text = ' '.join(lines[1:])  # Join all lines to form the cleaned_text\n",
        "\n",
        "    # Preprocessing: Remove unwanted characters using regex\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
        "\n",
        "    \"#1 Calculate positive score\"\n",
        "    positive_score = sum(1 for word in cleaned_text.split() if word.lower() in positive_words)\n",
        "\n",
        "    \"#2 Calculate negative score (multiply by -1)\"\n",
        "    negative_score = -1 * sum(-1 for word in cleaned_text.split() if word.lower() in negative_words)\n",
        "\n",
        "    \"#3 Calculate polarity score\"\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "\n",
        "    \"#4 Calculate subjective score\"\n",
        "    subjective_score = (positive_score + negative_score) / (len(cleaned_text.split()) + 0.000001)\n",
        "\n",
        "    \"#5 Avg Sentence Length\"\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(cleaned_text)\n",
        "    total_sentences = len(sentences)\n",
        "    avg_sentence_length = len(cleaned_text.split()) / total_sentences\n",
        "\n",
        "    \"#6 Percentage of Complex Words\"\n",
        "    # Define the complexity criteria (syllable threshold)\n",
        "    syllable_threshold = 2\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(cleaned_text)\n",
        "    # Initialize the count of complex words and syllables\n",
        "    complex_word_count = 0\n",
        "    total_syllables = 0\n",
        "\n",
        "    # Iterate through each word and check if it meets the complexity criteria\n",
        "    for word in words:\n",
        "        # Count syllables using the custom function\n",
        "        syllables = count_syllables(word)\n",
        "\n",
        "        # Handle exceptions for words ending with \"es\" or \"ed\"\n",
        "        if word.endswith('es') or word.endswith('ed'):\n",
        "            syllables -= 1\n",
        "\n",
        "        if syllables >= syllable_threshold:\n",
        "            complex_word_count += 1\n",
        "\n",
        "        total_syllables += syllables\n",
        "\n",
        "    #Percent of complex words\n",
        "    POCW = 100 * complex_word_count / len(cleaned_text.split())\n",
        "\n",
        "    \"#7 Fog Index\"\n",
        "    fog_index = 0.4 * (avg_sentence_length + POCW)\n",
        "\n",
        "    \"#8 Calculate the average number of words per sentence\"\n",
        "    AWPS = len(words) / (total_sentences or 1)  # Handle zero division\n",
        "\n",
        "    \"#9 Complex word count\"\n",
        "    complex_word_count = complex_word_count\n",
        "\n",
        "    \"#10 Count the number of words and sentences\"\n",
        "    num_words = len(words)\n",
        "\n",
        "    \"#11 Syllable per word\"\n",
        "    scpw = total_syllables/num_words\n",
        "\n",
        "    \"#12 Count personal pronouns using regex\"\n",
        "    # Find personal pronoun matches\n",
        "    personal_pronoun_matches = personal_pronoun_pattern.findall(text)\n",
        "    personal_pronoun_count = len(personal_pronoun_matches)\n",
        "\n",
        "    \"#13 Average word length\"\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    average_word_length = total_characters / len(words) if len(words) > 0 else 0\n",
        "\n",
        "    # Append the result to the results list\n",
        "    results.append((url_id, title, positive_score, negative_score, polarity_score, subjective_score,\n",
        "                    avg_sentence_length, POCW, fog_index, AWPS, complex_word_count, num_words, scpw,\n",
        "                    average_word_length))\n",
        "\n",
        "# Create a DataFrame from the results list\n",
        "df = pd.DataFrame(results, columns=['URL_ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score',\n",
        "                                    'Subjectivity Score', 'Avg Sentence Length', 'Percentage of complex words',\n",
        "                                    'Fog Index', \"Average Number of Words per Sentence\", 'Complex word count', 'Word Count', 'Syllable per word',\n",
        "                                    'Avg Word Length'])\n",
        "\n",
        "# Save the DataFrame to an XLSX file\n",
        "output_file = 'OUTPUT.xlsx'\n",
        "df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f'Scores saved to {output_file}')\n"
      ],
      "metadata": {
        "id": "47TWo72vh7k8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63959e1d-dd72-4299-caff-d638eeb63499"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores saved to OUTPUT.xlsx\n"
          ]
        }
      ]
    }
  ]
}